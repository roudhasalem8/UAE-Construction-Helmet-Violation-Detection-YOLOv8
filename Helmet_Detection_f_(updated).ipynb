{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/roudhasalem8/UAE-Construction-Helmet-Violation-Detection-YOLOv8/blob/main/Helmet_Detection_f_(updated).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9c8o6YeYmna8"
      },
      "outputs": [],
      "source": [
        "# Smart Helmet Detection System for UAE Construction Sites\n",
        "#**Real-Time Safety Compliance Using YOLOv8 | UAE Vision 2030 & SDG 8**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCqRJb_VmpgP",
        "outputId": "4d7baab5-3abe-4885-fe3c-84d9bc22bd25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[90m‚ï∫\u001b[0m\u001b[90m‚îÅ‚îÅ\u001b[0m \u001b[32m1.0/1.1 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.2/207.2 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hAll dependencies installed!\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install ultralytics roboflow supervision albumentations opencv-python-headless matplotlib -q\n",
        "\n",
        "\n",
        "print(\"All dependencies installed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kOffefrNmty3",
        "outputId": "026534f6-1e26-4931-bcbd-06945050034a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ‚úÖ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "Libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as numpy\n",
        "import matplotlib.pyplot as plt\n",
        "from ultralytics import YOLO\n",
        "from roboflow import Roboflow\n",
        "import supervision as sv\n",
        "import albumentations as A\n",
        "from IPython.display import Image, display\n",
        "import torch\n",
        "\n",
        "print(\"Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8NPpF7ymzSO",
        "outputId": "02318450-fce2-4acb-cf7f-9cac1a91890b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Copied dataset zip from Drive to Colab: /content/Hard Hat Workers.v3-v3.yolov8.zip\n",
            "Extracting zip... please wait (~1 min for 785MB)\n",
            "‚úÖ Extracted to: temp_extracted\n",
            "Items in extract: ['README.dataset.txt', 'README.roboflow.txt', 'data.yaml', 'test', 'train']\n",
            "‚ö†Ô∏è Could not find dataset root. Moving all contents...\n",
            "\n",
            "üéØ SUCCESS: Dataset ready at /content/dataset\n",
            "\n",
            "üìÅ Structure:\n",
            "total 28\n",
            "drwxr-xr-x 4 root root 4096 Nov 15 21:20 .\n",
            "drwxr-xr-x 1 root root 4096 Nov 15 21:20 ..\n",
            "-rw-r--r-- 1 root root  279 Nov 15 21:20 data.yaml\n",
            "-rw-r--r-- 1 root root 4025 Nov 15 21:20 README.dataset.txt\n",
            "-rw-r--r-- 1 root root 1162 Nov 15 21:20 README.roboflow.txt\n",
            "drwxr-xr-x 4 root root 4096 Nov 15 21:20 test\n",
            "drwxr-xr-x 4 root root 4096 Nov 15 21:20 train\n",
            "\n",
            "üñºÔ∏è Sample Images:\n",
            "000001_jpg.rf.063823aaf332982f3e950651f452a290.jpg\n",
            "000001_jpg.rf.0f6de3cb4d6f36215d869bbf4d230577.jpg\n",
            "000001_jpg.rf.88f7e69ab4248790a4806cef2d956ecc.jpg\n",
            "000002_jpg.rf.4b6174111f467b069d247047a2b5e689.jpg\n",
            "000002_jpg.rf.892f6349a85d0e0909143da672e27f44.jpg\n",
            "\n",
            "üè∑Ô∏è Sample Labels:\n",
            "==> dataset/train/labels/000001_jpg.rf.063823aaf332982f3e950651f452a290.txt <==\n",
            "1 0.3983333333333333 0.3075 0.11166666666666666 0.16333333333333333\n",
            "1 0.24166666666666667 0.4375 0.05 0.08166666666666667\n",
            "==> dataset/train/labels/000001_jpg.rf.0f6de3cb4d6f36215d869bbf4d230577.txt <==\n",
            "1 0.605 0.30916666666666665 0.115 0.165\n"
          ]
        }
      ],
      "source": [
        "# === FINAL FIX: Clean & Extract YOLOv8 Dataset from Google Drive ===\n",
        "\n",
        "import zipfile\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# === STEP 0: Mount Google Drive ===\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# === STEP 1: Set your dataset path from Drive ===\n",
        "# üëá Replace the path inside quotes with your actual file path in Drive\n",
        "drive_zip_path = \"/content/drive/MyDrive/Hard Hat Workers.v3-v3.yolov8.zip\"\n",
        "\n",
        "# Copy it to /content for faster read/write\n",
        "local_zip_path = \"/content/Hard Hat Workers.v3-v3.yolov8.zip\"\n",
        "shutil.copy(drive_zip_path, local_zip_path)\n",
        "print(f\"Copied dataset zip from Drive to Colab: {local_zip_path}\")\n",
        "\n",
        "# === STEP 2: Remove any old/bad 'dataset' folder ===\n",
        "if os.path.exists(\"dataset\"):\n",
        "    if os.path.isfile(\"dataset\"):\n",
        "        print(\"Removing old 'dataset' file...\")\n",
        "        os.remove(\"dataset\")\n",
        "    else:\n",
        "        print(\"Removing old 'dataset' folder...\")\n",
        "        shutil.rmtree(\"dataset\")\n",
        "\n",
        "# === STEP 3: Extract to temp folder ===\n",
        "extract_dir = \"temp_extracted\"\n",
        "if os.path.exists(extract_dir):\n",
        "    shutil.rmtree(extract_dir)\n",
        "os.makedirs(extract_dir, exist_ok=True)\n",
        "\n",
        "print(\"Extracting zip... please wait (~1 min for 785MB)\")\n",
        "with zipfile.ZipFile(local_zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)\n",
        "\n",
        "print(\"‚úÖ Extracted to:\", extract_dir)\n",
        "\n",
        "# === STEP 4: Find the actual dataset folder inside ===\n",
        "extracted_items = os.listdir(extract_dir)\n",
        "print(\"Items in extract:\", extracted_items)\n",
        "\n",
        "dataset_root = None\n",
        "for item in extracted_items:\n",
        "    item_path = os.path.join(extract_dir, item)\n",
        "    if os.path.isdir(item_path):\n",
        "        subdirs = os.listdir(item_path)\n",
        "        if any(x in subdirs for x in ['train', 'valid', 'test']):\n",
        "            dataset_root = item_path\n",
        "            break\n",
        "\n",
        "if dataset_root is None:\n",
        "    print(\"‚ö†Ô∏è Could not find dataset root. Moving all contents...\")\n",
        "    dataset_root = extract_dir  # fallback\n",
        "\n",
        "# === STEP 5: Move to /content/dataset ===\n",
        "shutil.move(dataset_root, \"dataset\")\n",
        "if dataset_root != extract_dir:\n",
        "    shutil.rmtree(extract_dir)\n",
        "\n",
        "dataset_location = \"dataset\"\n",
        "\n",
        "print(\"\\nüéØ SUCCESS: Dataset ready at /content/dataset\")\n",
        "print(\"\\nüìÅ Structure:\")\n",
        "!ls -la dataset/\n",
        "\n",
        "print(\"\\nüñºÔ∏è Sample Images:\")\n",
        "!ls dataset/train/images 2>/dev/null | head -5 || echo \"No images\"\n",
        "\n",
        "print(\"\\nüè∑Ô∏è Sample Labels:\")\n",
        "!head -n 2 dataset/train/labels/*.txt 2>/dev/null | head -5 || echo \"No labels\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JV-meUjVtmtM",
        "outputId": "163ac288-6bd5-4ad4-8af7-eb7ba84c3503"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "valid folder created\n",
            "   images : 1766\n",
            "   labels : 1766\n"
          ]
        }
      ],
      "source": [
        "# === STEP 1: CREATE valid FOLDER ===\n",
        "import os, shutil\n",
        "\n",
        "src_img = \"dataset/test/images\"\n",
        "src_lbl = \"dataset/test/labels\"\n",
        "dst_img = \"dataset/valid/images\"\n",
        "dst_lbl = \"dataset/valid/labels\"\n",
        "\n",
        "os.makedirs(dst_img, exist_ok=True)\n",
        "os.makedirs(dst_lbl, exist_ok=True)\n",
        "\n",
        "# copy everything from test ‚Üí valid\n",
        "!cp -r {src_img}/* {dst_img}/ 2>/dev/null\n",
        "!cp -r {src_lbl}/* {dst_lbl}/ 2>/dev/null\n",
        "\n",
        "print(f\"valid folder created\")\n",
        "print(f\"   images : {len(os.listdir(dst_img))}\")\n",
        "print(f\"   labels : {len(os.listdir(dst_lbl))}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NKpAoidm0Re",
        "outputId": "6a1d5875-f37c-47b7-e86e-f6588bab8f89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data.yaml written to /content/dataset/data.yaml\n"
          ]
        }
      ],
      "source": [
        "# === STEP 2: WRITE data.yaml (FULL PATH) ===\n",
        "data_yaml = \"\"\"\n",
        "path: /content/dataset\n",
        "train: train/images\n",
        "val: valid/images\n",
        "test: test/images\n",
        "\n",
        "names:\n",
        "  0: person\n",
        "  1: head\n",
        "  2: helmet\n",
        "\"\"\"\n",
        "\n",
        "with open(\"/content/dataset/data.yaml\", \"w\") as f:\n",
        "    f.write(data_yaml.strip() + \"\\n\")\n",
        "\n",
        "print(\"data.yaml written to /content/dataset/data.yaml\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tdr0iVC0trIU"
      },
      "outputs": [],
      "source": [
        "# === STEP 3: REMOVE WRONG data.yaml (if any) ===\n",
        "!rm -f /content/data.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uf22WayBvwY-",
        "outputId": "c22a8938-b0c3-43cb-a408-bc9af4a5b478"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU available: False\n",
            "GPU name: N/A\n"
          ]
        }
      ],
      "source": [
        "# Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí **GPU**\n",
        "# Then run this cell to confirm\n",
        "import torch\n",
        "print(\"GPU available:\", torch.cuda.is_available())\n",
        "print(\"GPU name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"N/A\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8-g0La1qkaU",
        "outputId": "b7adfe57-17af-4d99-f3a5-1c1a87d63cb4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting to ensure all existing labels are set to 'helmet' (ID 1)...\n",
            "Finished. Modified 0 label files to Class ID 1 ('helmet').\n",
            "--------------------------------------------------\n",
            "The primary challenge now is the 'head' (ID 0) class...\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# --- Configuration (Check your Roboflow download path) ---\n",
        "LABEL_DIR = '/content/dataset/train/labels'\n",
        "# Assuming your existing labels are all using one ID, e.g., '1',\n",
        "# but they represent the head WITH a helmet.\n",
        "OLD_ID = '1'\n",
        "NEW_ID = '0' # Assuming 'head' is 0 and 'helmet' is 1, let's fix the data based on logic.\n",
        "\n",
        "# Based on your data.yaml: names: ['head', 'helmet']\n",
        "# We must assume the current labels are for the object of interest: 'helmet' (ID 1).\n",
        "# If your current labels are all ID '1', and that represents 'helmet',\n",
        "# then you only need to create the 'head' (ID 0) labels.\n",
        "\n",
        "# Since you can't modify all labels, we'll try a different tactic:\n",
        "# ASSUME ALL EXISTING BOUNDING BOXES ARE FOR THE 'helmet' CLASS (ID 1).\n",
        "\n",
        "# --- Fix 1: Ensure all labels are correctly set to 'helmet' (ID 1) ---\n",
        "print(f\"Starting to ensure all existing labels are set to 'helmet' (ID 1)...\")\n",
        "count = 0\n",
        "\n",
        "for filename in os.listdir(LABEL_DIR):\n",
        "    if filename.endswith(\".txt\"):\n",
        "        filepath = os.path.join(LABEL_DIR, filename)\n",
        "\n",
        "        with open(filepath, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        new_lines = []\n",
        "        modified = False\n",
        "\n",
        "        for line in lines:\n",
        "            parts = line.strip().split()\n",
        "            if parts and parts[0] != '1': # If the ID is NOT 1 (helmet)\n",
        "                # Change it to 1 (helmet)\n",
        "                parts[0] = '1'\n",
        "                new_lines.append(' '.join(parts) + '\\n')\n",
        "                modified = True\n",
        "            else:\n",
        "                new_lines.append(line)\n",
        "\n",
        "        if modified:\n",
        "            count += 1\n",
        "            with open(filepath, 'w') as f:\n",
        "                f.writelines(new_lines)\n",
        "\n",
        "print(f\"Finished. Modified {count} label files to Class ID 1 ('helmet').\")\n",
        "print(\"-\" * 50)\n",
        "print(\"The primary challenge now is the 'head' (ID 0) class...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rAWr8rEm2re",
        "outputId": "ff06e0d7-ecaa-473b-94bc-928d5d2c9781"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "UAE Augmentation Pipeline READY ‚Äì ZERO WARNINGS\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "UserWarning: Argument(s) 'var_limit' are not valid for transform GaussNoise\n",
            "UserWarning: Argument(s) 'fog_coef_lower, fog_coef_upper' are not valid for transform RandomFog\n",
            "UserWarning: Argument(s) 'angle_lower, angle_upper' are not valid for transform RandomSunFlare\n"
          ]
        }
      ],
      "source": [
        "\n",
        "transform = A.Compose([\n",
        "    A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.6),\n",
        "    A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n",
        "    A.GaussNoise(var_limit=(10.0, 60.0), p=0.4),\n",
        "    A.MotionBlur(blur_limit=5, p=0.3),\n",
        "    A.RandomFog(fog_coef_lower=0.1, fog_coef_upper=0.3, alpha_coef=0.2, p=0.3),\n",
        "    A.RandomSunFlare(flare_roi=(0, 0, 1, 0.5), angle_lower=0.0, angle_upper=1.0, p=0.2),\n",
        "    A.RandomShadow(shadow_dimension=5, shadow_roi=(0, 0.5, 1, 1), p=0.3),\n",
        "    A.RandomRain(drop_length=15, drop_width=1, blur_value=3, p=0.1),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.Rotate(limit=20, p=0.5),\n",
        "    A.RandomScale(scale_limit=0.2, p=0.5),\n",
        "],\n",
        "    bbox_params=A.BboxParams(format='yolo',\n",
        "                             label_fields=['class_labels'],\n",
        "                             min_visibility=0.3)\n",
        ")\n",
        "\n",
        "print(\"UAE Augmentation Pipeline READY ‚Äì ZERO WARNINGS\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Etjxs_Iim4te",
        "outputId": "cea9083e-bee0-4713-bdcc-0d7829787325"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics 8.3.228 üöÄ Python-3.12.12 torch-2.8.0+cu126 CPU (AMD EPYC 7B12)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=True, auto_augment=randaugment, batch=32, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.1, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/dataset/data.yaml, degrees=15, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=3, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.001, lrf=0.1, mask_ratio=4, max_det=300, mixup=0.1, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=uae_helmet_yolov8s, nbs=64, nms=False, opset=None, optimize=False, optimizer=AdamW, overlap_mask=True, patience=15, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=runs/uae_helmet, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/content/runs/uae_helmet/uae_helmet_yolov8s, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=2.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "Overriding model.yaml nc=80 with nc=3\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    751897  ultralytics.nn.modules.head.Detect           [3, [64, 128, 256]]           \n",
            "Model summary: 129 layers, 3,011,433 parameters, 3,011,417 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 1150.3¬±537.6 MB/s, size: 52.5 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/dataset/train/labels... 15807 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 15807/15807 2.9Kit/s 5.4s\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/dataset/train/labels.cache\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 1889.1¬±883.0 MB/s, size: 48.5 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/dataset/valid/labels... 1766 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1766/1766 3.0Kit/s 0.6s\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/dataset/valid/labels.cache\n",
            "Plotting labels to /content/runs/uae_helmet/uae_helmet_yolov8s/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001, momentum=0.937) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 0 dataloader workers\n",
            "Logging results to \u001b[1m/content/runs/uae_helmet/uae_helmet_yolov8s\u001b[0m\n",
            "Starting training for 3 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K        1/3         0G      1.726      1.782      1.535        193        640: 36% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 180/494 19.8s/it 59:35<1:43:52"
          ]
        }
      ],
      "source": [
        "# === STEP 5: TRAIN YOLOv8s ===\n",
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO(\"yolov8n.pt\")\n",
        "\n",
        "results = model.train(\n",
        "    data=\"/content/dataset/data.yaml\",   # <-- FULL PATH\n",
        "    epochs=30,\n",
        "    imgsz=640,\n",
        "    batch=32,\n",
        "    name=\"uae_helmet_yolov8s\",\n",
        "    project=\"runs/uae_helmet\",\n",
        "    exist_ok=True,\n",
        "    patience=15,\n",
        "    optimizer='AdamW',\n",
        "    lr0=0.001,\n",
        "    lrf=0.1,\n",
        "    momentum=0.937,\n",
        "    weight_decay=0.0005,\n",
        "    warmup_epochs=3,\n",
        "    box=7.5, cls=0.5, dfl=1.5,\n",
        "    hsv_h=0.015, hsv_s=0.7, hsv_v=0.4,\n",
        "    degrees=15, translate=0.1, scale=0.5,\n",
        "    shear=2.0, perspective=0.0,\n",
        "    flipud=0.0, fliplr=0.5,\n",
        "    mosaic=1.0, mixup=0.1, copy_paste=0.1,\n",
        "    auto_augment='randaugment',\n",
        "    erasing=0.4,\n",
        "    save=True, plots=True, val=True,\n",
        "    augment=True               # built‚Äëin YOLO augmentations\n",
        ")\n",
        "\n",
        "print(\"Training finished!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1FIqSOmm6jm"
      },
      "outputs": [],
      "source": [
        "# Validate the trained model\n",
        "metrics = model.val()\n",
        "\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(f\"FINAL EVALUATION RESULTS\")\n",
        "print(f\"{'='*50}\")\n",
        "print(f\"mAP@50-95: {metrics.box.map:.3f}\")\n",
        "print(f\"mAP@50:    {metrics.box.map50:.3f}\")\n",
        "print(f\"mAP@75:    {metrics.box.map75:.3f}\")\n",
        "print(f\"Precision: {metrics.box.mp:.3f}\")\n",
        "print(f\"Recall:    {metrics.box.mr:.3f}\")\n",
        "print(f\"F1-Score:  {2 * metrics.box.mp * metrics.box.mr / (metrics.box.mp + metrics.box.mr + 1e-16):.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqM_5ufOm7vm"
      },
      "outputs": [],
      "source": [
        "# -------------------------------------------------\n",
        "# 1. Load the best checkpoint\n",
        "# -------------------------------------------------\n",
        "from ultralytics import YOLO\n",
        "import supervision as sv   # for drawing boxes / labels\n",
        "\n",
        "best_model = YOLO(\"runs/uae_helmet/uae_helmet_yolov8s/weights/best.pt\")\n",
        "print(\"Best model loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "naWZErEsm9mO"
      },
      "outputs": [],
      "source": [
        "# -------------------------------------------------\n",
        "# 2. Validation ‚Äì prints mAP, Precision, Recall, F1\n",
        "# -------------------------------------------------\n",
        "metrics = best_model.val(data=\"/content/dataset/data.yaml\")   # same yaml as training\n",
        "\n",
        "print(\"\\n\" + \"=\"*55)\n",
        "print(\"FINAL EVALUATION RESULTS\")\n",
        "print(\"=\"*55)\n",
        "print(f\"mAP@50-95 : {metrics.box.map:.3f}\")\n",
        "print(f\"mAP@50    : {metrics.box.map50:.3f}\")\n",
        "print(f\"mAP@75    : {metrics.box.map75:.3f}\")\n",
        "print(f\"Precision : {metrics.box.mp:.3f}\")\n",
        "print(f\"Recall    : {metrics.box.mr:.3f}\")\n",
        "print(f\"F1‚ÄëScore  : {2*metrics.box.mp*metrics.box.mr/(metrics.box.mp+metrics.box.mr+1e-16):.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jz7YVFhnAl2"
      },
      "outputs": [],
      "source": [
        "# -------------------------------------------------\n",
        "# 3. Download a realistic UAE construction picture\n",
        "# -------------------------------------------------\n",
        "#!wget -q \"https://upload.wikimedia.org/wikipedia/commons/2/2d/Construction_workers_road.jpg\" -O test_uae.jpg\n",
        "#print(\"Test image saved as test_uae.jpg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5dxvZm8uPCi"
      },
      "outputs": [],
      "source": [
        "# -------------------------------------------------\n",
        "# 4. Run inference & draw boxes + class‚Äëconfidence\n",
        "# -------------------------------------------------\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# inference\n",
        "results = best_model(\"test2.jpg\")[0]\n",
        "\n",
        "# supervision helpers\n",
        "detections = sv.Detections.from_ultralytics(results)\n",
        "\n",
        "# labels: class name + confidence\n",
        "labels = [\n",
        "    f\"{best_model.names[int(cls)]} {conf:.2f}\"\n",
        "    for cls, conf in zip(detections.class_id, detections.confidence)\n",
        "]\n",
        "\n",
        "# draw\n",
        "annotator = sv.BoxAnnotator(thickness=2)\n",
        "label_annotator = sv.LabelAnnotator(text_thickness=1, text_scale=0.5)\n",
        "\n",
        "annotated = annotator.annotate(scene=results.orig_img.copy(), detections=detections)\n",
        "annotated = label_annotator.annotate(scene=annotated, detections=detections, labels=labels)\n",
        "\n",
        "sv.plot_image(annotated, (12, 8))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FEe1OpXguQuC"
      },
      "outputs": [],
      "source": [
        "# -------------------------------------------------\n",
        "# 5. Detect workers whose head is NOT covered by a helmet\n",
        "# -------------------------------------------------\n",
        "detections = sv.Detections.from_ultralytics(results)\n",
        "img = results.orig_img.copy()\n",
        "\n",
        "helmet_boxes = []\n",
        "head_boxes   = []\n",
        "\n",
        "# separate helmet & head detections\n",
        "for cls, box in zip(detections.class_id, detections.xyxy):\n",
        "    if best_model.names[int(cls)] == \"helmet\":\n",
        "        helmet_boxes.append(box)\n",
        "    elif best_model.names[int(cls)] == \"head\":\n",
        "        head_boxes.append(box)\n",
        "\n",
        "no_helmet_workers = []\n",
        "\n",
        "for head_box in head_boxes:\n",
        "    hx1, hy1, hx2, hy2 = head_box\n",
        "    head_cx = (hx1 + hx2) / 2\n",
        "    head_cy = (hy1 + hy2) / 2\n",
        "\n",
        "    has_helmet = False\n",
        "    for helm_box in helmet_boxes:\n",
        "        hx1h, hy1h, hx2h, hy2h = helm_box\n",
        "        if (hx1h < head_cx < hx2h) and (hy1h - 60 < head_cy < hy2h + 60):\n",
        "            has_helmet = True\n",
        "            break\n",
        "\n",
        "    if not has_helmet:\n",
        "        no_helmet_workers.append(head_box)\n",
        "\n",
        "# draw alerts\n",
        "alert_img = img.copy()\n",
        "for box in no_helmet_workers:\n",
        "    x1, y1, x2, y2 = map(int, box)\n",
        "    cv2.rectangle(alert_img, (x1, y1), (x2, y2), (0, 0, 255), 3)\n",
        "    cv2.putText(alert_img, \"NO HELMET!\", (x1, y1 - 10),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
        "\n",
        "print(f\"ALERT: {len(no_helmet_workers)} worker(s) WITHOUT helmet\")\n",
        "sv.plot_image(alert_img, (12, 8))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQlL5sFKuSWi"
      },
      "outputs": [],
      "source": [
        "# -------------------------------------------------\n",
        "# 6. Export ONNX (edge) + TorchScript (fallback)\n",
        "# -------------------------------------------------\n",
        "best_model.export(format=\"onnx\", imgsz=640, simplify=True)\n",
        "best_model.export(format=\"torchscript\")\n",
        "\n",
        "print(\"Model exported\")\n",
        "!ls -lh runs/uae_helmet/uae_helmet_yolov8s/weights/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9KHcYVN1uUEy"
      },
      "outputs": [],
      "source": [
        "# -------------------------------------------------\n",
        "# 7. Zip & download everything you need for the report\n",
        "# -------------------------------------------------\n",
        "!zip -r uae_helmet_results.zip \\\n",
        "    runs/uae_helmet/uae_helmet_yolov8s/weights/best.pt \\\n",
        "    runs/uae_helmet/uae_helmet_yolov8s/weights/best.onnx \\\n",
        "    runs/uae_helmet/uae_helmet_yolov8s/weights/best.torchscript \\\n",
        "    test_uae.jpg\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"uae_helmet_results.zip\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}